
Experimenting with biological, physical, and artificial systems to generate a more informative dynamical model is a well-established practice in modern science. 
Traditional methods for modelling physical systems are based on laws of physics that are based either on empirical relationships or intuition. 
For systems that evolve with time, physical laws yield mathematical equations that govern how the quantities evolve with time. 
However, our world is much more complex than that which can be distilled into elegant equations. We do not fully understand many of the more complex systems, nor do they provide us with a good physical intuition of the underlying principles governing system dynamics. To complicate this, the underlying systems we observe often display sensitive dependence on initial conditions despite having highly similar initial conditions. This is because differing orbits diverge quickly and to such an extent that it becomes seemingly impossible to retrace their steps back to the original conditions. Even the presence of usually `negligible' computational noise or measurement error renders long-term pointwise prediction infeasible. We encounter many difficultiesin the process of modelling such systems:

\vspace{-8mm}
\begin{enumerate}[noitemsep, label=\roman*.]
  \item One may not have access to the complete states of the systems
  \item The system may be described by functions that behave wildly because their graphs have a wild oscillatory behaviour, i.e., a large functional complexity.
\end{enumerate}


Models derived primarily from data can be classified into three categories: 
\vspace{-8mm}
\begin{enumerate}[noitemsep, label=\roman*.]
  \item  Interpretable models (i.e., they establish relationships between internal physical mechanisms), 
  \item Partially interpretable models capturing some modes of the dynamics, 
  \item Non-interpretable models, defined as such mainly due to the fact that they are defined on a different phase space that is usually high-dimensional. 
\end{enumerate}

Examples in the literature attempting to forecast data from such systems have tried many different approaches with differing degrees of success. 
When we have complete access to states of the system, an ordinary differential equation can be obtained from data (\cite{brunton2016discovering, champion2019data} and~\cite{small2002modeling,xu2006modeling}) wherein one could approximate the vector field by a library of functions to obtain interpretable models. 


Recent partially interpretable models available in the literature have been based on the Koopman operator (see~\cite{koopman1932dynamical,budivsic2012applied}) to employ observables mapping the data onto a higher-dimensional space. This makes the dynamics in the higher-dimensional space more amenable for approximation by a linear transformation. Such methods do not guarantee exact reconstructions for nonlinear models, and in practice provide poor long-term consistency for a large class of chaotic dynamical systems~\cite{Supp}.

The non-interpretable models include the delay embedding and the machine learning algorithms. For example, one could learn a system conjugate to the underlying system by applying the Takens delay embedding method of delay-coordinates~\cite{takens1981detecting} when one has `good' observations from the system; this learnt system could then be used to forecast the observed data. 
Takens delay embedding theorem~\cite{takens1981detecting} and its various generalisations (see,~\cite{sauer1991embedology, stark1999delay, gutman2018embedding}) establishes the learnability of a system constructed by concatenating sufficiently large previous time-series observations of a dynamical system into a vector (called delay coordinates). This then confirms the existence of a map on the space of delay coordinates equivalent (or topologically conjugate) to the underlying map from which the observed time-series was first obtained. Although topological conjugacy guarantees an alternate representation of the underlying system, the quality still depends on numerous parameters, making the comprehension of the dynamics unreliable (see, [?]). One reason for this fragility is that the embedded attractor in the reconstruction space is not always an attractor of the map learnt in the reconstruction space, despite unquestionably being an invariant set. When the embedded object is not explicitly known to be an attractor (as explained in Chapter~\ref{ch3}), it can cause predictions to fail.

Practically, the application of Takens embedding involves learning a map through some technique, and consequently one wishes that these would have low functional complexity\cite{manjunath2021universal}, i.e., functions with fewer oscillatory graphs. On the other hand, pure machine learning methodology processes temporal information (like the echo state networks~\cite{Manju_ESP, Manju_IEEE, grigoryeva2018echo}) by mapping data onto a higher dimensional space for further processing. Although they perform  well on forecasting some dynamical systems, they fail completely on others (\ednote{Ask for resources here}) as there is often no guarantee that the right function was learnt during training.

This project involves implementing and analysing non-interpretable models that can guarantee exact reconstruction. 
The project work concerns the study and implementation of a method (\cite{manjunath2021universal}) that incorporates learning a function by mapping the data on to a higher dimensional space using what is called a driven dynamical system (See Chapter~ref{ch.4}). With a clear understanding of how the data is mapped onto the higher dimensional space, the method then permits the learning of a dynamical system topologically conjugate to that of the underlying system. Instead of linear regression as in the training of echo state networks, deep learning methods are employed to learn the correct function. With slight modifications to the implementation in the paper (\cite{manjunath2021universal}), we show that one can construct accurate non-interpretable models with the ability to reconstruct attractors from more hard-to-forecast systems like the double pendulum. (The forecasting of the time-series from a double pendulum has not been reported before.) Moreover, we also demonstrate that long-term statistical consistency is preserved.

\emph{By solidifying the mathematical underpinnings of our theory, we hope to guarantee the ability to construct models with predictive power ranging from molecular biology to neuroscience in the near future. (We can modify this sentence when the report is completed.)}

The more intricate details of proofs are referred to where relevant throughout. This report is organised as follows: 
\newline In Chapter~\ref{ch2}, we recall the definition of a discrete-time dynamical system, how a discrete-system arises from a flow of an ODE and then proceed to define the inverse-limit space and topological conjugacy of autonomous systems. 
\newline Chapter~\ref{ch3} introduces the problem of forecasting dynamical systems, states the Takens delay embedding theorem, and discusses various issues faced while forecasting. 
\newline In Chapter~\ref{ch4}, a driven dynamical system is defined and discuss the properties of a specific class of driven dynamical systems that we make use of in this project.
\newline Finally, in Chapter~\ref{ch5} we show the implementation of these forecasting methods, and conclusions are provided in Chapter~\ref{ch6}.


